#!/usr/bin/env python3
"""
RAGç³»ç»Ÿé…ç½®ç®¡ç†æ¨¡å—

æä¾›å®Œæ•´çš„é…ç½®ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- åˆ†å±‚é…ç½®æ¨¡åž‹
- ç±»åž‹å®‰å…¨éªŒè¯
- çŽ¯å¢ƒå˜é‡æ”¯æŒ
- é…ç½®çƒ­é‡è½½
- é…ç½®ç¼“å­˜
- é…ç½®è¿ç§»
"""

import os
import json
import logging
import hashlib
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List, Union, Tuple
from pathlib import Path
from functools import lru_cache
from contextlib import contextmanager
import yaml
from pydantic import BaseModel, Field, validator, ConfigDict, field_validator
import threading
import time
from enum import Enum

logger = logging.getLogger(__name__)

class MetricType(str, Enum):
    """è·ç¦»åº¦é‡ç±»åž‹æžšä¸¾"""
    IP = "IP"
    L2 = "L2"
    COSINE = "COSINE"

class ConsistencyLevel(str, Enum):
    """ä¸€è‡´æ€§çº§åˆ«æžšä¸¾"""
    STRONG = "Strong"
    BOUNDED = "Bounded"
    EVENTUALLY = "Eventually"
    SESSION = "Session"

class IndexType(str, Enum):
    """ç´¢å¼•ç±»åž‹æžšä¸¾"""
    IVF_FLAT = "IVF_FLAT"
    IVF_SQ8 = "IVF_SQ8"
    IVF_PQ = "IVF_PQ"
    HNSW = "HNSW"
    FLAT = "FLAT"

class LogLevel(str, Enum):
    """æ—¥å¿—çº§åˆ«æžšä¸¾"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

class APIConfig(BaseModel):
    """APIé…ç½®æ¨¡åž‹"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    openai_api_key: str = Field(default="", description="OpenAI APIå¯†é’¥")
    openai_base_url: str = Field(default="http://10.172.10.103:11434/v1", description="OpenAIå…¼å®¹APIåŸºç¡€URL")
    timeout: int = Field(default=30, ge=1, le=300, description="APIè¶…æ—¶æ—¶é—´(ç§’)")
    max_retries: int = Field(default=3, ge=0, le=10, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    retry_delay: float = Field(default=1.0, ge=0.1, le=60.0, description="é‡è¯•å»¶è¿Ÿ(ç§’)")
    
    @field_validator('openai_base_url')
    def validate_url(cls, v, info):
        if not v.startswith(('http://', 'https://')):
            raise ValueError("URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´")
        return v

class DatabaseConfig(BaseModel):
    """æ•°æ®åº“é…ç½®æ¨¡åž‹"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    milvus_uri: str = Field(default="http://10.172.10.100:19530", description="MilvusæœåŠ¡URI")
    collection_name: str = Field(default="qwen3_embedding_rag", description="é›†åˆåç§°")
    embedding_dim: int = Field(default=1024, ge=1, le=8192, description="åµŒå…¥å‘é‡ç»´åº¦")
    metric_type: MetricType = Field(default=MetricType.IP, description="è·ç¦»åº¦é‡ç±»åž‹")
    consistency_level: ConsistencyLevel = Field(default=ConsistencyLevel.STRONG, description="ä¸€è‡´æ€§çº§åˆ«")
    index_type: IndexType = Field(default=IndexType.IVF_FLAT, description="ç´¢å¼•ç±»åž‹")
    nlist: int = Field(default=1024, ge=1, description="èšç±»æ•°é‡")
    nprobe: int = Field(default=16, ge=1, description="æœç´¢èšç±»æ•°")
    
    @field_validator('milvus_uri')
    def validate_milvus_uri(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError("Milvus URIå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´")
        return v
    
    @field_validator('collection_name')
    def validate_collection_name(cls, v):
        if not v or len(v) > 64:
            raise ValueError("é›†åˆåç§°ä¸èƒ½ä¸ºç©ºä¸”é•¿åº¦ä¸èƒ½è¶…è¿‡64ä¸ªå­—ç¬¦")
        if not v.replace('_', '').replace('-', '').isalnum():
            raise ValueError("é›†åˆåç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿žå­—ç¬¦")
        return v

class ModelsConfig(BaseModel):
    """æ¨¡åž‹é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    embedding_model: str = Field(default="hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0", description="åµŒå…¥æ¨¡åž‹")
    reranker_model: str = Field(default="qwen3:4b", description="é‡æŽ’åºæ¨¡åž‹")
    llm_model: str = Field(default="qwen3:4b", description="å¤§è¯­è¨€æ¨¡åž‹")
    embedding_batch_size: int = Field(default=32, ge=1, le=128, description="åµŒå…¥æ‰¹å¤„ç†å¤§å°")
    llm_max_tokens: int = Field(default=2048, ge=1, le=8192, description="LLMæœ€å¤§tokenæ•°")
    llm_temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="LLMæ¸©åº¦å‚æ•°")
    
    @field_validator('embedding_model', 'reranker_model', 'llm_model')
    def validate_model_name(cls, v):
        if not v or len(v) > 256:
            raise ValueError("æ¨¡åž‹åç§°ä¸èƒ½ä¸ºç©ºä¸”é•¿åº¦ä¸èƒ½è¶…è¿‡256ä¸ªå­—ç¬¦")
        return v

class DataConfig(BaseModel):
    """æ•°æ®å¤„ç†é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    data_path_glob: str = Field(default="milvus_docs/en/faq/*.md", description="æ•°æ®æ–‡ä»¶è·¯å¾„æ¨¡å¼")
    chunk_size: int = Field(default=1000, ge=100, le=10000, description="æ–‡æœ¬åˆ†å—å¤§å°")
    chunk_overlap: int = Field(default=200, ge=0, le=5000, description="åˆ†å—é‡å å¤§å°")
    supported_formats: List[str] = Field(default=[".md", ".txt", ".pdf"], description="æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
    encoding: str = Field(default="utf-8", description="æ–‡ä»¶ç¼–ç ")
    
    @field_validator('supported_formats')
    def validate_formats(cls, v):
        if not v:
            raise ValueError("æ”¯æŒçš„æ–‡ä»¶æ ¼å¼åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        for fmt in v:
            if not fmt.startswith('.'):
                raise ValueError(f"æ–‡ä»¶æ ¼å¼å¿…é¡»ä»¥.å¼€å¤´: {fmt}")
        return v
    
    @field_validator('encoding')
    def validate_encoding(cls, v):
        try:
            "test".encode(v)
            return v
        except LookupError:
            raise ValueError(f"ä¸æ”¯æŒçš„ç¼–ç æ ¼å¼: {v}")

class SearchConfig(BaseModel):
    """æœç´¢é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    search_limit: int = Field(default=10, ge=1, le=100, description="æœç´¢ç»“æžœæ•°é‡é™åˆ¶")
    rerank_top_k: int = Field(default=3, ge=1, le=20, description="é‡æŽ’åºtop-k")
    similarity_threshold: float = Field(default=0.7, ge=0.0, le=1.0, description="ç›¸ä¼¼åº¦é˜ˆå€¼")
    enable_rerank: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨é‡æŽ’åº")
    enable_hybrid_search: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨æ··åˆæœç´¢")

class PerformanceConfig(BaseModel):
    """æ€§èƒ½é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    cache_size: int = Field(default=1000, ge=100, le=10000, description="ç¼“å­˜å¤§å°")
    max_workers: int = Field(default=4, ge=1, le=32, description="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°")
    batch_size: int = Field(default=32, ge=1, le=256, description="æ‰¹å¤„ç†å¤§å°")
    enable_gpu: bool = Field(default=False, description="æ˜¯å¦å¯ç”¨GPU")
    gpu_memory_fraction: float = Field(default=0.8, ge=0.1, le=1.0, description="GPUå†…å­˜ä½¿ç”¨æ¯”ä¾‹")
    
    @field_validator('max_workers')
    def validate_max_workers(cls, v):
        import multiprocessing
        max_cpus = multiprocessing.cpu_count()
        if v > max_cpus * 2:
            logger.warning(f"å·¥ä½œçº¿ç¨‹æ•°({v})è¶…è¿‡CPUæ ¸å¿ƒæ•°çš„2å€({max_cpus * 2})ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        return v

class LoggingConfig(BaseModel):
    """æ—¥å¿—é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    log_level: LogLevel = Field(default=LogLevel.INFO, description="æ—¥å¿—çº§åˆ«")
    log_file: str = Field(default="logs/rag_system.log", description="æ—¥å¿—æ–‡ä»¶è·¯å¾„")
    max_log_size: str = Field(default="10MB", description="æœ€å¤§æ—¥å¿—æ–‡ä»¶å¤§å°")
    backup_count: int = Field(default=5, ge=0, le=20, description="å¤‡ä»½æ–‡ä»¶æ•°é‡")
    log_format: str = Field(default="%(asctime)s - %(name)s - %(levelname)s - %(message)s", description="æ—¥å¿—æ ¼å¼")
    
    @field_validator('max_log_size')
    def validate_log_size(cls, v):
        import re
        pattern = r'^(\d+)(KB|MB|GB)$'
        if not re.match(pattern, v.upper()):
            raise ValueError("æ—¥å¿—å¤§å°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºæ•°å­—+å•ä½(KB/MB/GB)")
        return v.upper()

class OutputConfig(BaseModel):
    """è¾“å‡ºé…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    output_dir: str = Field(default="answers", description="è¾“å‡ºç›®å½•")
    save_intermediate_results: bool = Field(default=True, description="æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æžœ")
    include_metadata: bool = Field(default=True, description="æ˜¯å¦åŒ…å«å…ƒæ•°æ®")
    output_format: str = Field(default="txt", description="è¾“å‡ºæ ¼å¼")
    
    @field_validator('output_format')
    def validate_output_format(cls, v):
        supported_formats = ['txt', 'json', 'yaml', 'csv']
        if v.lower() not in supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {v}ï¼Œæ”¯æŒ: {supported_formats}")
        return v.lower()

class PromptsConfig(BaseModel):
    """æç¤ºè¯é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    system_prompt: str = Field(
        default="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºŽæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å‡†ç¡®å›žç­”é—®é¢˜ã€‚å¦‚æžœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜Žç¡®è¯´æ˜Žã€‚",
        description="ç³»ç»Ÿæç¤ºè¯"
    )
    query_prompt: str = Field(
        default="åŸºäºŽä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›žç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š",
        description="æŸ¥è¯¢æç¤ºè¯"
    )
    rerank_prompt: str = Field(
        default="è¯·å¯¹ä»¥ä¸‹å€™é€‰ç­”æ¡ˆè¿›è¡Œé‡æ–°æŽ’åºï¼Œé€‰æ‹©æœ€ç›¸å…³çš„ç­”æ¡ˆï¼š\n\n{answers}\n\né—®é¢˜ï¼š{question}",
        description="é‡æŽ’åºæç¤ºè¯"
    )
    
    @field_validator('system_prompt', 'query_prompt', 'rerank_prompt')
    def validate_prompt_length(cls, v):
        if len(v) > 10000:
            raise ValueError("æç¤ºè¯é•¿åº¦ä¸èƒ½è¶…è¿‡10000ä¸ªå­—ç¬¦")
        return v

class SecurityConfig(BaseModel):
    """å®‰å…¨é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    enable_ssl_verification: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨SSLéªŒè¯")
    api_key_rotation_days: int = Field(default=30, ge=1, le=365, description="APIå¯†é’¥è½®æ¢å¤©æ•°")
    max_request_size: str = Field(default="10MB", description="æœ€å¤§è¯·æ±‚å¤§å°")
    rate_limit_per_minute: int = Field(default=60, ge=1, le=1000, description="æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶")
    
    @field_validator('max_request_size')
    def validate_request_size(cls, v):
        import re
        pattern = r'^(\d+)(KB|MB|GB)$'
        if not re.match(pattern, v.upper()):
            raise ValueError("è¯·æ±‚å¤§å°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºæ•°å­—+å•ä½(KB/MB/GB)")
        return v.upper()

class MonitoringConfig(BaseModel):
    """ç›‘æŽ§é…ç½®"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    enable_metrics: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨æŒ‡æ ‡æ”¶é›†")
    metrics_port: int = Field(default=8000, ge=1024, le=65535, description="æŒ‡æ ‡æœåŠ¡ç«¯å£")
    health_check_interval: int = Field(default=30, ge=5, le=300, description="å¥åº·æ£€æŸ¥é—´éš”(ç§’)")
    performance_monitoring: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æŽ§")

class RAGConfigModel(BaseModel):
    """RAGç³»ç»Ÿå®Œæ•´é…ç½®æ¨¡åž‹"""
    model_config = ConfigDict(extra="forbid", validate_assignment=True)
    
    api: APIConfig = Field(default_factory=APIConfig)
    database: DatabaseConfig = Field(default_factory=DatabaseConfig)
    models: ModelsConfig = Field(default_factory=ModelsConfig)
    data: DataConfig = Field(default_factory=DataConfig)
    search: SearchConfig = Field(default_factory=SearchConfig)
    performance: PerformanceConfig = Field(default_factory=PerformanceConfig)
    logging: LoggingConfig = Field(default_factory=LoggingConfig)
    output: OutputConfig = Field(default_factory=OutputConfig)
    prompts: PromptsConfig = Field(default_factory=PromptsConfig)
    security: SecurityConfig = Field(default_factory=SecurityConfig)
    monitoring: MonitoringConfig = Field(default_factory=MonitoringConfig)

    @field_validator('data')
    def validate_chunk_overlap(cls, v):
        if v.chunk_overlap >= v.chunk_size:
            raise ValueError("chunk_overlapå¿…é¡»å°äºŽchunk_size")
        return v

    @field_validator('search')
    def validate_rerank_top_k(cls, v):
        if v.rerank_top_k > v.search_limit:
            raise ValueError("rerank_top_kä¸èƒ½å¤§äºŽsearch_limit")
        return v

@dataclass
class ConfigCache:
    """é…ç½®ç¼“å­˜"""
    data: Dict[str, Any]
    hash: str
    timestamp: float
    file_path: str

class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®ç±» - æ”¯æŒé…ç½®éªŒè¯ã€çŽ¯å¢ƒå˜é‡å’Œçƒ­é‡è½½"""
    
    def __init__(self, config_file: Optional[str] = None):
        """åˆå§‹åŒ–é…ç½®"""
        self._config: RAGConfigModel = RAGConfigModel()
        self._config_file: Optional[str] = config_file
        self._last_modified: float = 0
        self._lock: threading.RLock = threading.RLock()
        self._cache: Optional[ConfigCache] = None
        self._env_mappings: Dict[str, str] = {
            'api.openai_api_key': 'OPENAI_API_KEY',
            'api.openai_base_url': 'OPENAI_BASE_URL',
            'database.milvus_uri': 'MILVUS_URI',
            'database.collection_name': 'MILVUS_COLLECTION',
            'logging.log_level': 'LOG_LEVEL',
        }
        
        # åˆå§‹åŒ–åŽå¤„ç†
        self._load_from_env()
        self._validate_config()
        
        # å¦‚æžœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œåˆ™åŠ è½½
        if config_file:
            self.load_from_file(config_file)
    
    def _load_from_env(self) -> None:
        """ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        for config_path, env_var in self._env_mappings.items():
            env_value = os.getenv(env_var)
            if env_value:
                self._set_nested_value(config_path, env_value)
    
    def _set_nested_value(self, path: str, value: Any) -> None:
        """è®¾ç½®åµŒå¥—é…ç½®å€¼"""
        keys = path.split('.')
        obj = self._config
        for key in keys[:-1]:
            obj = getattr(obj, key)
        setattr(obj, keys[-1], value)
    
    def _validate_config(self) -> None:
        """éªŒè¯é…ç½®"""
        try:
            self._config = RAGConfigModel.model_validate(self._config.model_dump())
        except Exception as e:
            logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
            raise
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""
    
    def _should_reload(self, config_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½é…ç½®"""
        if not config_path.exists():
            return False
        
        current_mtime = config_path.stat().st_mtime
        current_hash = self._calculate_file_hash(config_path)
        
        # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´å’Œå“ˆå¸Œå€¼
        if (current_mtime > self._last_modified or 
            (self._cache and current_hash != self._cache.hash)):
            return True
        
        return False
    
    @lru_cache(maxsize=128)
    def _parse_config_file(self, file_path: Path) -> Dict[str, Any]:
        """è§£æžé…ç½®æ–‡ä»¶ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        with open(file_path, 'r', encoding='utf-8') as f:
            if file_path.suffix.lower() in ('.yaml', '.yml'):
                return yaml.safe_load(f)
            else:
                return json.load(f)
    
    def load_from_file(self, config_file: str) -> bool:
        """ä»Žæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            with self._lock:
                config_path = Path(config_file)
                if not config_path.exists():
                    logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
                    return False
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
                if not self._should_reload(config_path):
                    return True
                
                # è§£æžé…ç½®æ–‡ä»¶
                config_data = self._parse_config_file(config_path)
                
                # ç§»é™¤æ³¨é‡Šå­—æ®µ
                config_data = self._remove_comments(config_data)
                
                # æ›´æ–°é…ç½®
                self._config = RAGConfigModel.model_validate(config_data)
                self._config_file = config_file
                self._last_modified = config_path.stat().st_mtime
                
                # æ›´æ–°ç¼“å­˜
                self._cache = ConfigCache(
                    data=config_data,
                    hash=self._calculate_file_hash(config_path),
                    timestamp=time.time(),
                    file_path=str(config_path)
                )
                
                # é‡æ–°åŠ è½½çŽ¯å¢ƒå˜é‡
                self._load_from_env()
                
                logger.info(f"é…ç½®å·²ä»Žæ–‡ä»¶åŠ è½½: {config_file}")
                return True
                
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _remove_comments(self, data: Any) -> Any:
        """ç§»é™¤é…ç½®ä¸­çš„æ³¨é‡Šå­—æ®µ"""
        if isinstance(data, dict):
            return {k: self._remove_comments(v) for k, v in data.items() 
                   if not k.startswith('_')}
        elif isinstance(data, list):
            return [self._remove_comments(item) for item in data]
        else:
            return data
    
    def save_to_file(self, config_file: str, include_comments: bool = True) -> bool:
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with self._lock:
                config_path = Path(config_file)
                config_path.parent.mkdir(parents=True, exist_ok=True)
                
                config_data = self._config.model_dump()
                
                # æ·»åŠ æ³¨é‡Š
                if include_comments:
                    config_data = self._add_comments(config_data)
                
                with open(config_path, 'w', encoding='utf-8') as f:
                    if config_path.suffix.lower() in ('.yaml', '.yml'):
                        yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)
                    else:
                        json.dump(config_data, f, indent=2, ensure_ascii=False)
                
                logger.info(f"é…ç½®å·²ä¿å­˜åˆ°æ–‡ä»¶: {config_file}")
                return True
                
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _add_comments(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ é…ç½®æ³¨é‡Š"""
        comments = {
            "_comment": "RAGç³»ç»Ÿé…ç½®æ–‡ä»¶ - åŸºäºŽQwen3 Embeddingå’ŒMilvuså‘é‡æ•°æ®åº“",
            "_version": "1.0.0",
            "_description": "æ£€ç´¢å¢žå¼ºç”Ÿæˆç³»ç»Ÿé…ç½®"
        }
        
        result = {**comments, **data}
        
        # ä¸ºæ¯ä¸ªéƒ¨åˆ†æ·»åŠ æ³¨é‡Š
        section_comments = {
            "api": "APIç›¸å…³é…ç½®",
            "database": "Milvuså‘é‡æ•°æ®åº“é…ç½®",
            "models": "AIæ¨¡åž‹é…ç½®",
            "data": "æ•°æ®å¤„ç†é…ç½®",
            "search": "æœç´¢ç›¸å…³é…ç½®",
            "performance": "æ€§èƒ½ä¼˜åŒ–é…ç½®",
            "logging": "æ—¥å¿—é…ç½®",
            "output": "è¾“å‡ºé…ç½®",
            "prompts": "æç¤ºè¯é…ç½®",
            "security": "å®‰å…¨é…ç½®",
            "monitoring": "ç›‘æŽ§é…ç½®"
        }
        
        for section, comment in section_comments.items():
            if section in result:
                result[section] = {"_comment": comment, **result[section]}
        
        return result
    
    def get(self, key: str, default: Any = None) -> Any:
        """èŽ·å–é…ç½®å€¼"""
        try:
            keys = key.split('.')
            obj = self._config
            for k in keys:
                obj = getattr(obj, k)
            return obj
        except AttributeError:
            return default
    
    def set(self, key: str, value: Any) -> bool:
        """è®¾ç½®é…ç½®å€¼"""
        try:
            with self._lock:
                self._set_nested_value(key, value)
                self._validate_config()
                return True
        except Exception as e:
            logger.error(f"è®¾ç½®é…ç½®å¤±è´¥: {e}")
            return False
    
    def reload(self) -> bool:
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        if self._config_file:
            return self.load_from_file(self._config_file)
        return False
    
    @contextmanager
    def temporary_config(self, **kwargs):
        """ä¸´æ—¶é…ç½®ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        original_values = {}
        try:
            # ä¿å­˜åŽŸå§‹å€¼
            for key, value in kwargs.items():
                # å°†åŒä¸‹åˆ’çº¿è½¬æ¢ä¸ºç‚¹åˆ†éš”ç¬¦
                config_key = key.replace('__', '.')
                original_values[config_key] = self.get(config_key)
                self.set(config_key, value)
            yield self
        finally:
            # æ¢å¤åŽŸå§‹å€¼
            for key, value in original_values.items():
                self.set(key, value)
    
    def print_config(self, detailed: bool = False) -> None:
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("RAG ç³»ç»Ÿé…ç½®")
        logger.info("=" * 60)
        logger.info(f"é…ç½®æ–‡ä»¶: {self._config_file or 'é»˜è®¤é…ç½®'}")
        
        if detailed:
            # è¯¦ç»†é…ç½®ä¿¡æ¯
            config_dict = self._config.model_dump()
            for section, section_data in config_dict.items():
                logger.info(f"\nðŸ”§ {section.upper()}é…ç½®:")
                for key, value in section_data.items():
                    logger.info(f"  {key}: {value}")
        else:
            # ç®€è¦é…ç½®ä¿¡æ¯
            logger.info(f"åµŒå…¥æ¨¡åž‹: {self._config.models.embedding_model}")
            logger.info(f"é‡æŽ’åºæ¨¡åž‹: {self._config.models.reranker_model}")
            logger.info(f"LLMæ¨¡åž‹: {self._config.models.llm_model}")
            logger.info(f"Milvus URI: {self._config.database.milvus_uri}")
            logger.info(f"æ•°æ®è·¯å¾„: {self._config.data.data_path_glob}")
            logger.info(f"åˆ†å—å¤§å°: {self._config.data.chunk_size}, é‡å : {self._config.data.chunk_overlap}")
            logger.info(f"æ‰¹å¤„ç†: {self._config.performance.batch_size}, ç¼“å­˜: {self._config.performance.cache_size}")
            logger.info(f"å¹¶å‘æ•°: {self._config.performance.max_workers}")
            logger.info(f"æœç´¢é™åˆ¶: {self._config.search.search_limit}, é‡æŽ’åº: {self._config.search.rerank_top_k}")
        
        logger.info("=" * 60)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return self._config.model_dump()
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        try:
            self._validate_config()
            return True
        except Exception as e:
            logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_diff(self, other_config: 'RAGConfig') -> Dict[str, Tuple[Any, Any]]:
        """èŽ·å–ä¸Žå¦ä¸€ä¸ªé…ç½®çš„å·®å¼‚"""
        diff = {}
        current_dict = self.to_dict()
        other_dict = other_config.to_dict()
        
        def compare_dicts(dict1: Dict, dict2: Dict, prefix: str = ""):
            for key in set(dict1.keys()) | set(dict2.keys()):
                full_key = f"{prefix}.{key}" if prefix else key
                val1 = dict1.get(key)
                val2 = dict2.get(key)
                
                if isinstance(val1, dict) and isinstance(val2, dict):
                    compare_dicts(val1, val2, full_key)
                elif val1 != val2:
                    diff[full_key] = (val1, val2)
        
        compare_dicts(current_dict, other_dict)
        return diff
    
    def migrate_from_old_format(self, old_config: Dict[str, Any]) -> bool:
        """ä»Žæ—§æ ¼å¼è¿ç§»é…ç½®"""
        try:
            migration_map = {
                'embedding_model': 'models.embedding_model',
                'reranker_model': 'models.reranker_model',
                'llm_model': 'models.llm_model',
                'milvus_uri': 'database.milvus_uri',
                'collection_name': 'database.collection_name',
                'embedding_dim': 'database.embedding_dim',
                'data_path_glob': 'data.data_path_glob',
                'chunk_size': 'data.chunk_size',
                'chunk_overlap': 'data.chunk_overlap',
                'search_limit': 'search.search_limit',
                'rerank_top_k': 'search.rerank_top_k',
                'batch_size': 'performance.batch_size',
                'max_workers': 'performance.max_workers',
                'cache_size': 'performance.cache_size',
                'system_prompt': 'prompts.system_prompt',
            }
            
            for old_key, new_key in migration_map.items():
                if old_key in old_config:
                    self.set(new_key, old_config[old_key])
            
            logger.info("é…ç½®è¿ç§»å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"é…ç½®è¿ç§»å¤±è´¥: {e}")
            return False
    
    # ä¾¿æ·å±žæ€§è®¿é—®
    @property
    def api(self) -> APIConfig:
        return self._config.api
    
    @property
    def database(self) -> DatabaseConfig:
        return self._config.database
    
    @property
    def models(self) -> ModelsConfig:
        return self._config.models
    
    @property
    def data(self) -> DataConfig:
        return self._config.data
    
    @property
    def search(self) -> SearchConfig:
        return self._config.search
    
    @property
    def performance(self) -> PerformanceConfig:
        return self._config.performance
    
    @property
    def logging(self) -> LoggingConfig:
        return self._config.logging
    
    @property
    def output(self) -> OutputConfig:
        return self._config.output
    
    @property
    def prompts(self) -> PromptsConfig:
        return self._config.prompts
    
    @property
    def security(self) -> SecurityConfig:
        return self._config.security
    
    @property
    def monitoring(self) -> MonitoringConfig:
        return self._config.monitoring
    
    def __repr__(self) -> str:
        return f"RAGConfig(config_file='{self._config_file}', valid={self.validate()})"
    
    def __str__(self) -> str:
        return f"RAGConfig(valid={self.validate()})" 